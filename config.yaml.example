# GitHub to Qdrant Sync Configuration Template
# Copy this file to config.yaml and update with your settings
# Sensitive data should be stored in environment variables (see .env.example)

# ============================================================
# GitHub Repository Configuration
# ============================================================
github:
  repository_url: https://github.com/your-org/your-repo.git
  branch: main
  name: ""  # Optional: Human-readable name (e.g., "User Documentation")
  clone_depth: 1
  cleanup_after_processing: true
  token: ${GITHUB_TOKEN}  # Optional: for private repos

# ============================================================
# Embedding Provider Selection
# ============================================================
# Choose one: azure_openai, mistral_ai, sentence_transformers
embedding_provider: mistral_ai

# ============================================================
# Azure OpenAI Configuration
# ============================================================
azure_openai:
  api_key: ${AZURE_OPENAI_API_KEY}
  endpoint: ${AZURE_OPENAI_ENDPOINT}
  model: text-embedding-3-large  # or text-embedding-ada-002, text-embedding-3-small
  api_version: "2024-02-01"
  dimensions: 3072  # Output vector dimensions (supports dimension reduction)
                    # text-embedding-3-large: native 3072, can reduce to 1536, 1024, 512, etc.
                    # text-embedding-3-small: native 1536, can reduce to 512, 256, etc.
                    # text-embedding-ada-002: fixed 1536 (no reduction)

# ============================================================
# Mistral AI Configuration
# ============================================================
mistral_ai:
  api_key: ${MISTRAL_API_KEY}
  model: codestral-embed  # or mistral-embed
  dimensions: 3072  # Output vector dimensions
                    # codestral-embed: max 3072
                    # mistral-embed: fixed 1024
  api_base: https://api.mistral.ai/v1

# ============================================================
# Sentence Transformers (Local) Configuration
# ============================================================
sentence_transformers:
  # Model options:
  # - sentence-transformers/all-MiniLM-L6-v2 (384 dimensions)
  # - intfloat/multilingual-e5-large (1024 dimensions)
  model: intfloat/multilingual-e5-large
  dimensions: 1024  # Auto-detected from model, but can be overridden

# ============================================================
# Qdrant Vector Database Configuration
# ============================================================
qdrant:
  url: ${QDRANT_URL}  # https://your-cluster.cloud.qdrant.io or http://localhost:6333
  api_key: ${QDRANT_API_KEY}
  collection_name: your-collection-name
  vector_size: 1536  # Must match embedding dimensions
  distance: Cosine  # Options: Cosine, Euclidean, Dot
  vector_name: null  # null for unnamed vectors, "model-name" for named vectors
  recreate_collection: false  # WARNING: true will delete existing collection!
  
  # Connection method (auto-detected if not specified)
  # Options: auto, reverse_proxy, direct, url
  connection_method: auto
  timeout: 30

  # Payload indexing (optional, recommended for large collections)
  #
  # This creates Qdrant payload indexes for faster filtered search / grouping.
  # Works with both payload layouts:
  # - payload.metadata_structure: nested -> indexes use paths like "metadata.repository"
  # - payload.metadata_structure: flat   -> indexes use paths like "repository"
  payload_indexes:
    enabled: false
    apply_to_existing_collections: true
    fields:
      - name: repo_id
        type: keyword
      - name: file_id
        type: keyword
      - name: file_upload_id
        type: keyword
      - name: repository
        type: keyword
      - name: source
        type: keyword
      - name: source_type
        type: keyword
      - name: extraction_method
        type: keyword
      - name: content_hash
        type: keyword
      - name: file_hash
        type: keyword
      - name: embedding_provider
        type: keyword
      - name: embedding_model
        type: keyword
      - name: page_number
        type: integer

# ============================================================
# Document Processing Configuration
# ============================================================
processing:
  # File processing mode
  file_mode: all_text  # Options: all_text, markdown_only

  # Text chunking configuration
  chunk_size: 1000  # Characters per chunk (500-2000 recommended)
  chunk_overlap: 200  # Overlap for context continuity
  chunking_strategy: recursive  # Options: recursive (default), token_recursive, semantic

  # Token-aware chunking (recommended for stable chunk sizes across models)
  # Only used when chunking_strategy is token_recursive (or token).
  chunk_size_tokens: 512
  chunk_overlap_tokens: 64
  tiktoken_encoding: cl100k_base
  
  # Markdown-specific extensions
  markdown_extensions:
    - .md
    - .markdown
    - .mdown
    - .mkd
    - .mdx
  
  # All text file extensions (for all_text mode)
  text_extensions:
    # Documentation formats
    - .md
    - .markdown
    - .txt
    - .rst
    
    # Programming languages
    - .py
    - .js
    - .ts
    - .jsx
    - .tsx
    - .java
    - .go
    - .rs
    - .php
    - .rb
    - .c
    - .cpp
    - .cs
    
    # Web technologies
    - .html
    - .css
    - .scss
    - .vue
    - .svelte
    
    # Configuration files
    - .json
    - .yaml
    - .yml
    - .toml
    - .ini
    - .env
    
    # Shell scripts
    - .sh
    - .bash
    
    # Build files
    - .dockerfile
    - .gitignore
    
    # Add more extensions as needed...
  
  # Exclude these patterns from processing
  exclude_patterns:
    - node_modules
    - .git
    - __pycache__
    - "*.pyc"
    - .DS_Store
    - dist
    - build
    - "*.min.js"
    - "*.min.css"
    - vendor

  # File processing mode (NEW in v0.3.3)
  # Controls how repository files are processed and stored:
  # - false: Process files individually (RECOMMENDED)
  #   * Maintains document boundaries for better context
  #   * Deterministic vector IDs prevent duplicates on re-runs
  #   * Better search quality and relevance per document
  #   * Proper source attribution for each file
  #   * Ideal for RAG applications and semantic search
  # - true: Combine all files (legacy mode)
  #   * Creates single large combined document
  #   * May lose file-level context and boundaries
  #   * Use only for compatibility with pre-v0.3.3 setups
  combine_documents: false

  # Incremental sync (optional)
  # If enabled, the pipeline will skip fully-ingested, unchanged files based on file_hash.
  track_file_changes: false
  # Legacy cleanup (advanced): if you are upgrading an existing collection that predates repo_id/file_id/file_upload_id,
  # you can temporarily enable this to delete old points by file_path. Leave disabled for shared-collection setups.
  legacy_cleanup_delete_by_file_path: false

  # Deduplication settings
  deduplication_enabled: true
  similarity_threshold: 0.95  # 0.9-0.98 recommended
  
  # Embedding generation settings
  embedding_batch_size: 50  # Optimized for Azure OpenAI
  max_retries: 3
  batch_delay_seconds: 1  # Prevent rate limiting

# ============================================================
# PDF Processing Configuration
# ============================================================
# Note: Cloud mode and image extraction require Mistral AI to be configured
pdf_processing:
  enabled: true
  mode: local  # Options: local, cloud (requires Mistral), hybrid (requires Mistral)
  extract_images: false  # Extract images from PDFs (requires Mistral)
  image_processing_mode: none  # Options: none, ocr, description (requires Mistral Vision API)

  # Local processing settings
  local:
    primary_method: pymupdf      # Options: pymupdf, pypdfloader
    fallback_method: pypdfloader # Options: pypdfloader, none
    min_text_per_page: 50        # Minimum characters to consider valid
    preserve_layout: true         # Maintain document layout

  # Cloud processing (Mistral OCR)
  cloud:
    enabled: false
    provider: mistral_ocr
    max_pages_per_doc: 100       # Cost control: max pages per PDF
    use_for_quality: false       # Use cloud even if local succeeds

  # Hybrid mode settings
  hybrid:
    prefer_local: true           # Try local methods first
    quality_threshold: 0.7       # Text density threshold (0-1)
    force_cloud_patterns:        # File patterns to always use cloud
      - "*scan*.pdf"
      - "*scanned*.pdf"
      - "*ocr*.pdf"

# ============================================================
# Output Configuration
# ============================================================
output:
  base_directory: markdown
  combined_filename: __combined_markdown.md
  preserve_structure: true

# ============================================================
# Payload Configuration
# ============================================================
payload:
  # Content field names - list all fields that should contain the document text
  # Configure based on your integration needs:
  # - content: Required for n8n
  # - page_content: Required for LangChain
  # - document: For MCP compatibility (optional)
  # - text: Alternative field name (optional)
  content_fields:
    - content      # Primary field for n8n
    - page_content # Required for LangChain
    # - document   # Uncomment for MCP compatibility
    # - text       # Uncomment if needed

  # Preview configuration
  preview_length: 200  # Characters for preview snippet (150-300 recommended)

  # Optional: Only include specified fields (reduces payload size)
  minimal_mode: false  # If true, only includes first 2 content fields

  # Optional: control which keys from chunk.metadata are copied into payload metadata.
  # If metadata_allowlist is set, only those keys will be copied (in addition to core fields).
  # Keys in metadata_denylist are never copied.
  # This helps keep the payload schema stable across runs.
  # metadata_allowlist:
  #   - file_path
  #   - file_hash
  metadata_denylist:
    - page_content
    - content
    - text
    - document

# ============================================================
# Retrieval (optional CLI helper)
# ============================================================
retrieval:
  top_k: 10
  fetch_k: 40
  max_chunks_per_file: 3
  parent_window: 2
  # Optional filters by metadata fields (use logical names; code will map to metadata.<field> if nested)
  # filters:
  #   repository: your-repo-name

  # Metadata structure (NEW in v0.3.4)
  # Options: "nested" (LangChain default) or "flat" (v0.3.2 behavior)
  # - nested: All metadata fields grouped under "metadata" key (recommended for LangChain compatibility)
  # - flat: All fields at root level (better for direct Qdrant filtering)
  metadata_structure: "nested"

# ============================================================
# Logging Configuration
# ============================================================
logging:
  level: INFO  # Options: DEBUG, INFO, WARNING, ERROR
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"